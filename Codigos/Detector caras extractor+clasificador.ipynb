{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de emociones\n",
    "### Modelización de problemas matemáticos UCM 2020 - Magnagement Solutions\n",
    "#### Equipo: YOLO - You Only Lose Once\n",
    "\n",
    "El siguiente programa permite, a partir de un modelo preentrenado y una fotografía, encontrar las caras de las personas presentes en la fotografía y predecir cuál es la emoción que expresan. \n",
    "\n",
    "SOLO sirve para cuando hay una CNN de la que sacamos las features y un clasificador \n",
    "\n",
    "Código basado en: https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importamos las librerías que vamos a utilizar\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "#Este código está porque los algoritmos de convolución los ejecutamos sobre una GPU y sin el código nos da problemas\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parámetros del programa\n",
    "ruta_imagen = 'Data/fotos_completas/asco.jpg'\n",
    "nombre_extractor = '62_26'\n",
    "n_capa = 13 #Número de capa del modelo de la que se extrae la vectorización\n",
    "\n",
    "nombre_clasificador = 'RF 62,26 Primera densa'\n",
    "resolucion_ancho = 960\n",
    "\n",
    "\n",
    "#Parámetros para dibujar en la imagen\n",
    "font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "bottomLeftCornerOfText = (10,500)\n",
    "fontScale              = 0.7\n",
    "fontColor              = (0, 0, 255)\n",
    "lineType               = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos tanto la CNN encargada de extraer las features como el clasificador\n",
    "extractor = keras.models.load_model(nombre_extractor)\n",
    "clasificador = pickle.load(open(nombre_clasificador, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el clasificador en cascada que utilizaremos para detectar las caras\n",
    "face_cascade = cv2.CascadeClassifier('Detectores/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Cargamos la imagen\n",
    "img = cv2.imread(ruta_imagen)\n",
    "\n",
    "#La reescalamos a un buen tamaño\n",
    "width = resolucion_ancho\n",
    "height = math.ceil(img.shape[0] * (resolucion_ancho/img.shape[1]))\n",
    "img = cv2.resize(img, (width, height))\n",
    "\n",
    "# Detectamos las caras\n",
    "faces = face_cascade.detectMultiScale(img, 1.1, 4)\n",
    "\n",
    "# Dibujamos alrededor de las caras un rectángulo\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), fontColor, lineType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFeature = K.function([extractor.layers[0].input, K.learning_phase()],\n",
    "                        [extractor.layers[n_capa].output])\n",
    "\n",
    "'''\n",
    "Función que extrae las features de una imagen\n",
    "Argumentos: X - vector con las imágenes\n",
    "Return: exTrain - vector con la vectorización de cada imagen\n",
    "'''\n",
    "def extract_features(img):\n",
    "    act = getFeature([img.reshape(1,48,48,1), 0])\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la imagen a gris (el predictor espera las imágenes en blanco y negro)\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Lista que empleamos para escribir las direcciones\n",
    "emociones = [\"enfado\", \"asco\", \"miedo\", \"felicidad\", \"neutral\", \"triste\", \"sorprendido\"]\n",
    "\n",
    "\n",
    "x_pr = np.empty((0,48,48,1))\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    # Recortamos la imagen\n",
    "    img_recortada = img_gray[y:y+h, x:x+w]\n",
    "    \n",
    "    # La reescalamos al tamaño que espera la red\n",
    "    img_recortada = cv2.resize(img_recortada, (48, 48))\n",
    "    img_recortada = np.reshape(img_recortada, (48,48,1))\n",
    "\n",
    "    #Predecimos el sentimiento de la cara\n",
    "    #1. Si usamos keras, tenemos que usar model.predict, y tendremos que sacar el argmax\n",
    "    features = extract_features(img_recortada)\n",
    "    \n",
    "    #2. Si usamos una SVM, Random Forest, ... nos saldrá la clase directamente\n",
    "    sol = clasificador.predict(features[0])\n",
    "    cv2.putText(img, emociones[int(sol)], (x, y - 5), font, fontScale,fontColor,lineType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimimos la foto\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
